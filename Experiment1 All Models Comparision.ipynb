{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import statistics\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "porter = PorterStemmer()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def vectorizeSeverityValue(value):\n",
    "    if value == 'Critical' or value == 'Blocker':\n",
    "        return 0\n",
    "    elif value == 'Minor' or value == 'Trivial':\n",
    "        return 1\n",
    "    elif value == 'Major':\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "def getTokenizedText(text):\n",
    "    text = str(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed.append(porter.stem(token))\n",
    "        stemmed.append(\" \")\n",
    "    stemmed = \"\".join(stemmed)\n",
    "    \n",
    "    #text cleaning\n",
    "    text_without_punctuation = [char for char in stemmed if char not in string.punctuation]\n",
    "    text_without_punctuation = ''.join(text_without_punctuation)\n",
    "\n",
    "    tokenized_text = [word for word in text_without_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "    tokenized_text = ' '.join(tokenized_text)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Logistic Regression Results ##\n",
      "{'0': {'precision': 0.7406417112299465, 'recall': 0.8978930307941653, 'f1-score': 0.8117216117216116, 'support': 617}, '1': {'precision': 0.6358381502890174, 'recall': 0.3618421052631579, 'f1-score': 0.4612159329140461, 'support': 304}, 'accuracy': 0.7209554831704669, 'macro avg': {'precision': 0.688239930759482, 'recall': 0.6298675680286616, 'f1-score': 0.6364687723178288, 'support': 921}, 'weighted avg': {'precision': 0.7060485705936355, 'recall': 0.7209554831704669, 'f1-score': 0.6960280977612425, 'support': 921}}\n",
      "[[554  63]\n",
      " [194 110]]\n",
      "## Random Forest Results ##\n",
      "{'0': {'precision': 0.727391874180865, 'recall': 0.899513776337115, 'f1-score': 0.8043478260869565, 'support': 617}, '1': {'precision': 0.6075949367088608, 'recall': 0.3157894736842105, 'f1-score': 0.41558441558441556, 'support': 304}, 'accuracy': 0.7068403908794788, 'macro avg': {'precision': 0.6674934054448629, 'recall': 0.6076516250106627, 'f1-score': 0.609966120835686, 'support': 921}, 'weighted avg': {'precision': 0.687849779727565, 'recall': 0.7068403908794788, 'f1-score': 0.6760263529134795, 'support': 921}}\n",
      "[[555  62]\n",
      " [208  96]]\n",
      "## Decision Tree Results ##\n",
      "{'0': {'precision': 0.7274247491638796, 'recall': 0.7050243111831442, 'f1-score': 0.7160493827160492, 'support': 617}, '1': {'precision': 0.43653250773993807, 'recall': 0.46381578947368424, 'f1-score': 0.44976076555023925, 'support': 304}, 'accuracy': 0.6254071661237784, 'macro avg': {'precision': 0.5819786284519088, 'recall': 0.5844200503284143, 'f1-score': 0.5829050741331443, 'support': 921}, 'weighted avg': {'precision': 0.631408200420255, 'recall': 0.6254071661237784, 'f1-score': 0.6281538999599078, 'support': 921}}\n",
      "[[435 182]\n",
      " [163 141]]\n",
      "## SVM (RBF) Results ##\n",
      "{'0': {'precision': 0.7342931937172775, 'recall': 0.9092382495948136, 'f1-score': 0.8124547429398986, 'support': 617}, '1': {'precision': 0.643312101910828, 'recall': 0.33223684210526316, 'f1-score': 0.43817787418655096, 'support': 304}, 'accuracy': 0.7187839305103149, 'macro avg': {'precision': 0.6888026478140528, 'recall': 0.6207375458500384, 'f1-score': 0.6253163085632247, 'support': 921}, 'weighted avg': {'precision': 0.7042625184630313, 'recall': 0.7187839305103149, 'f1-score': 0.6889149295837448, 'support': 921}}\n",
      "[[561  56]\n",
      " [203 101]]\n",
      "## SVM (linear) Results ##\n",
      "{'0': {'precision': 0.749648382559775, 'recall': 0.8638573743922204, 'f1-score': 0.802710843373494, 'support': 617}, '1': {'precision': 0.6, 'recall': 0.4144736842105263, 'f1-score': 0.49027237354085607, 'support': 304}, 'accuracy': 0.7155266015200868, 'macro avg': {'precision': 0.6748241912798875, 'recall': 0.6391655293013734, 'f1-score': 0.646491608457175, 'support': 921}, 'weighted avg': {'precision': 0.700253042388036, 'recall': 0.7155266015200868, 'f1-score': 0.699582401648063, 'support': 921}}\n",
      "[[533  84]\n",
      " [178 126]]\n",
      "## SVM (poly) Results ##\n",
      "{'0': {'precision': 0.6773480662983425, 'recall': 0.993517017828201, 'f1-score': 0.8055190538764784, 'support': 617}, '1': {'precision': 0.75, 'recall': 0.039473684210526314, 'f1-score': 0.075, 'support': 304}, 'accuracy': 0.6786102062975027, 'macro avg': {'precision': 0.7136740331491713, 'recall': 0.5164953510193636, 'f1-score': 0.44025952693823917, 'support': 921}, 'weighted avg': {'precision': 0.7013287262823859, 'recall': 0.6786102062975027, 'f1-score': 0.5643922434764247, 'support': 921}}\n",
      "[[613   4]\n",
      " [292  12]]\n",
      "## SVM (sigmoid) Results ##\n",
      "{'0': {'precision': 0.7493036211699164, 'recall': 0.8719611021069692, 'f1-score': 0.805992509363296, 'support': 617}, '1': {'precision': 0.6108374384236454, 'recall': 0.40789473684210525, 'f1-score': 0.4891518737672584, 'support': 304}, 'accuracy': 0.7187839305103149, 'macro avg': {'precision': 0.6800705297967808, 'recall': 0.6399279194745372, 'f1-score': 0.6475721915652772, 'support': 921}, 'weighted avg': {'precision': 0.7035992568323851, 'recall': 0.7187839305103149, 'f1-score': 0.7014110183522261, 'support': 921}}\n",
      "[[538  79]\n",
      " [180 124]]\n"
     ]
    }
   ],
   "source": [
    "# Log dataset\n",
    "logDataset = pd.read_csv('data/five_projects_log.csv')\n",
    "logDataset.dropna(subset=['title', 'description', 'new_priority'] , inplace=True)\n",
    "logDataset.new_priority = logDataset.new_priority.apply(vectorizeSeverityValue)\n",
    "logDataset.drop(logDataset[(logDataset['new_priority'] == 2) | (logDataset['new_priority'] == 3)].index, inplace=True)\n",
    "\n",
    "\n",
    "#print(logDataset.info())\n",
    "logDataset.title = logDataset.title.apply(getTokenizedText)\n",
    "logDataset.description = logDataset.description.apply(getTokenizedText)\n",
    "\n",
    "X = logDataset['title'].astype(str) + ' ' + logDataset['description'].astype(str)\n",
    "y = logDataset['new_priority']\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "print('## Logistic Regression Results ##')\n",
    "logisticRegression = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "logisticRegression.fit(X_train, y_train)\n",
    "y_pred = logisticRegression.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, output_dict=True))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "print('## Random Forest Results ##')\n",
    "randomForest = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "randomForest.fit(X_train, y_train)\n",
    "y_pred = randomForest.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, output_dict=True))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('## Decision Tree Results ##')\n",
    "decisionTree = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "decisionTree.fit(X_train, y_train)\n",
    "y_pred = decisionTree.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, output_dict=True))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('## SVM (RBF) Results ##')\n",
    "svmRBF = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "svmRBF.fit(X_train, y_train)\n",
    "y_pred = svmRBF.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, output_dict=True))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('## SVM (linear) Results ##')\n",
    "svmLinear = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "svmLinear.fit(X_train, y_train)\n",
    "y_pred = svmLinear.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, output_dict=True))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('## SVM (poly) Results ##')\n",
    "svmPoly = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC(kernel='poly'))\n",
    "])\n",
    "\n",
    "svmPoly.fit(X_train, y_train)\n",
    "y_pred = svmPoly.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, output_dict=True))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('## SVM (sigmoid) Results ##')\n",
    "svmSigmoid = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC(kernel='sigmoid'))\n",
    "])\n",
    "\n",
    "svmSigmoid.fit(X_train, y_train)\n",
    "y_pred = svmSigmoid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, output_dict=True))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2770a9e95224d0bbdfce8136c5f5e059cd58a6eef570d267a3ca0fbc5293076d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anaconda3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}