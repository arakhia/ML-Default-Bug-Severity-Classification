{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import statistics\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import statistics\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "porter = PorterStemmer()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def vectorizeSeverityValue(value):\n",
    "    if value == 'Critical' or value == 'Blocker':\n",
    "        return 0\n",
    "    elif value == 'Minor' or value == 'Trivial':\n",
    "        return 1\n",
    "    elif value == 'Major':\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "def getTokenizedText(text):\n",
    "    text = str(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed.append(porter.stem(token))\n",
    "        stemmed.append(\" \")\n",
    "    stemmed = \"\".join(stemmed)\n",
    "    \n",
    "    #text cleaning\n",
    "    text_without_punctuation = [char for char in stemmed if char not in string.punctuation]\n",
    "    text_without_punctuation = ''.join(text_without_punctuation)\n",
    "\n",
    "    tokenized_text = [word for word in text_without_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "    tokenized_text = ' '.join(tokenized_text)\n",
    "    return tokenized_text\n",
    "\n",
    "def experiment(X, y, classifier):\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    kfoldPrecision = []\n",
    "    kfoldRecall = []\n",
    "    kfoldF_Measures = []\n",
    "    for train, test in kfold.split(X, y):\n",
    "        X_train = X[train]\n",
    "        X_test = X[test]\n",
    "        y_train = y[train]\n",
    "        y_test = y[test]\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        kfoldPrecision.append(report['weighted avg']['precision'])\n",
    "        kfoldRecall.append(report['weighted avg']['recall'])\n",
    "        kfoldF_Measures.append(report['weighted avg']['f1-score'])\n",
    "\n",
    "    print('#### -- ####')\n",
    "    print('Average Precision')\n",
    "    print(statistics.mean(kfoldPrecision))\n",
    "    print('Average Recall')\n",
    "    print(statistics.mean(kfoldRecall))\n",
    "    print('Average F1-score')\n",
    "    print(statistics.mean(kfoldF_Measures))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### -- ####\n",
      "Average Precision\n",
      "0.698956172146779\n",
      "Average Recall\n",
      "0.7117263843648208\n",
      "Average F1-score\n",
      "0.6923003939577863\n",
      "#### -- ####\n",
      "Average Precision\n",
      "0.6895827105046974\n",
      "Average Recall\n",
      "0.7039087947882736\n",
      "Average F1-score\n",
      "0.6826478735420942\n",
      "#### -- ####\n",
      "Average Precision\n",
      "0.6979593181725446\n",
      "Average Recall\n",
      "0.7087947882736156\n",
      "Average F1-score\n",
      "0.6786119478041561\n"
     ]
    }
   ],
   "source": [
    "logDataset = pd.read_csv('data/five_projects_log.csv')\n",
    "logDataset.dropna(subset=['title', 'description', 'new_priority'] , inplace=True)\n",
    "logDataset.new_priority = logDataset.new_priority.apply(vectorizeSeverityValue)\n",
    "logDataset.drop(logDataset[(logDataset['new_priority'] == 2) | (logDataset['new_priority'] == 3)].index, inplace=True)\n",
    "\n",
    "logDataset = logDataset.reset_index(drop=True)\n",
    "\n",
    "logDataset.title = logDataset.title.apply(getTokenizedText)\n",
    "logDataset.description = logDataset.description.apply(getTokenizedText)\n",
    "\n",
    "X = logDataset['title'] + ' ' + logDataset['description']\n",
    "y = logDataset['new_priority']\n",
    "\n",
    "SVMLinear = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "experiment(X, y, SVMLinear)\n",
    "\n",
    "svmSigmoid = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC(kernel='sigmoid'))\n",
    "])\n",
    "\n",
    "experiment(X, y, svmSigmoid)\n",
    "\n",
    "logisticRegression = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "experiment(X, y, logisticRegression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2770a9e95224d0bbdfce8136c5f5e059cd58a6eef570d267a3ca0fbc5293076d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anaconda3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}